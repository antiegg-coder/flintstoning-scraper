import os, time, json, re
import gspread
from datetime import datetime
from oauth2client.service_account import ServiceAccountCredentials
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# [ì„¤ì •]
CONFIG = {
    "name": "ë ›í”Œ(Letspl)",
    "url": "https://letspl.me/project?location=KR00&type=00&recruitingType=all&jobD=0207",
    "gid": "1669656972"
}

# [ê³µí†µ] ì‹œíŠ¸ ì—°ê²° (GID ê¸°ë°˜ - íƒ­ ìˆœì„œ ë°”ë€Œì–´ë„ ë¬´ê´€)
def get_worksheet():
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    creds_dict = json.loads(os.environ['GOOGLE_CREDENTIALS'])
    creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
    client = gspread.authorize(creds)
    spreadsheet = client.open_by_url("https://docs.google.com/spreadsheets/d/1nKPVCZ6zAOfpqCjV6WfjkzCI55FA9r2yvi9XL3iIneo/edit")
    sheet = next((s for s in spreadsheet.worksheets() if str(s.id) == CONFIG["gid"]), None)
    if not sheet: raise Exception(f"{CONFIG['gid']} ì‹œíŠ¸ë¥¼ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    return sheet

# [ê³µí†µ] ë¸Œë¼ìš°ì € ì‹¤í–‰ (ë´‡ ë°©ì§€ í¬í•¨)
def get_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-blink-features=AutomationControlled")
    driver = webdriver.Chrome(options=options)
    driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {"source": "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"})
    return driver

# [ì „ìš©] ë°ì´í„° ìˆ˜ì§‘ (ì œëª© í•„í„°ë§ ê°•í™”)
def scrape_projects():
    driver = get_driver()
    new_data = []
    today = datetime.now().strftime("%Y-%m-%d")
    
    # ì§€ì—­ í‚¤ì›Œë“œ
    REGIONS = ["ì„œìš¸", "ê²½ê¸°", "ì¸ì²œ", "ëŒ€ì „", "ëŒ€êµ¬", "ë¶€ì‚°", "ê´‘ì£¼", "ìš¸ì‚°", "ì„¸ì¢…", "ê°•ì›", "ì¶©ë¶", "ì¶©ë‚¨", "ì „ë¶", "ì „ë‚¨", "ê²½ë¶", "ê²½ë‚¨", "ì œì£¼", "ì˜¨ë¼ì¸"]

    try:
        print(f"ğŸŒ {CONFIG['name']} ì ‘ì† ì¤‘...")
        driver.get(CONFIG["url"])
        wait = WebDriverWait(driver, 15)
        # í”„ë¡œì íŠ¸ ì¹´ë“œê°€ ë¡œë”©ë  ë•Œê¹Œì§€ ëŒ€ê¸°
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "a[href^='/project/']")))
        time.sleep(5) 
        
        cards = driver.find_elements(By.CSS_SELECTOR, "a[href^='/project/']")

        for elem in cards:
            try:
                href = elem.get_attribute("href")
                if not re.search(r'/project/\d+', href): continue
                
                # ----------------------------------------------------
                # [ë¡œì§ ë³€ê²½] íƒœê·¸ ê²½ë¡œë¥¼ ì¢í˜€ì„œ 'ì •í™•í•œ ì œëª©'ë§Œ ì°¾ê¸°
                # ----------------------------------------------------
                title = ""
                try:
                    # 1. h3 íƒœê·¸ë¥¼ ë¨¼ì € ì°¾ê³ 
                    h3_elem = elem.find_element(By.TAG_NAME, "h3")
                    # 2. h3 'ë‚´ë¶€'ì— ìˆëŠ” span ì¤‘ 'TitleTxt'ê°€ í¬í•¨ëœ í´ë˜ìŠ¤ë§Œ ì„ íƒ
                    # ì´ë ‡ê²Œ í•˜ë©´ h3 ë°”ê¹¥ì˜ 'ì£¼ëª©ì¤‘' ë°°ì§€ëŠ” ë¬¼ë¦¬ì ìœ¼ë¡œ ì¡í ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
                    title_elem = h3_elem.find_element(By.CSS_SELECTOR, "span[class*='TitleTxt']")
                    title = title_elem.text.strip()
                except:
                    # h3 êµ¬ì¡°ê°€ ì•„ë‹ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ìµœì†Œí•œì˜ ë°±ì—… (ì¹´ë“œ ì²« ì¤„)
                    pass

                # ìœ„ ë¡œì§ìœ¼ë¡œë„ ì œëª©ì„ ëª» ì°¾ì•˜ë‹¤ë©´ 2ë‹¨ê³„ í•„í„°ë§ìœ¼ë¡œ ë³´ì™„
                if not title or len(title) < 2:
                    # ê¸ˆì§€ì–´ ë¦¬ìŠ¤íŠ¸ (ë³´í—˜ìš©)
                    BAD_WORDS = ["íŒ”ë¡œìš°", "ìš°ì„ ë…¸ì¶œ", "ì£¼ëª©ì¤‘", "D-", "NEW"]
                    lines = elem.text.split('\n')
                    clean_lines = [l.strip() for l in lines if len(l.strip()) > 1 
                                   and not any(bad in l for bad in BAD_WORDS)]
                    if clean_lines: title = clean_lines[0]

                if not title or len(title) < 2: continue
                # ----------------------------------------------------

                loc = next((k for k in REGIONS if k in elem.text), "ë¯¸ì •")
                
                if not any(d['url'] == href for d in new_data):
                    new_data.append({'title': title, 'url': href, 'scraped_at': today, 'location': loc})
            except: continue
    finally: driver.quit()
    return new_data

# [ê³µí†µ] ìŠ¤ë§ˆíŠ¸ ì €ì¥ (ì»¬ëŸ¼ ìˆœì„œ ë°”ë€Œì–´ë„ í—¤ë” ì´ë¦„ìœ¼ë¡œ ìœ„ì¹˜ ì¶”ì )
def update_sheet(ws, data):
    if not data: return print(f"[{CONFIG['name']}] ìƒˆ ë°ì´í„° ì—†ìŒ")
    all_v = ws.get_all_values()
    headers = all_v[0] if all_v else ['title', 'url', 'scraped_at', 'status', 'location']
    
    # ì´ë¦„ìœ¼ë¡œ ì—´ ìœ„ì¹˜ ì°¾ê¸° (ë¡œì§ë² ì´ìŠ¤)
    col_map = {name: i for i, name in enumerate(headers)}
    if 'url' not in col_map: return print("âŒ 'url' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    existing_urls = {row[col_map['url']] for row in all_v[1:] if len(row) > col_map['url']}
    
    rows = []
    for item in data:
        if item['url'] in existing_urls: continue
        row = [''] * len(headers)
        for k, v in item.items():
            if k in col_map: row[col_map[k]] = v
        if 'status' in col_map: row[col_map['status']] = 'archived'
        rows.append(row)
    
    if rows:
        ws.append_rows(rows)
        print(f"ğŸ’¾ {CONFIG['name']} {len(rows)}ê±´ ì €ì¥ ì™„ë£Œ!")

if __name__ == "__main__":
    try:
        ws = get_worksheet()
        data = scrape_projects()
        update_sheet(ws, data)
    except Exception as e:
        print(f"ğŸš¨ {CONFIG['name']} ì‹¤í–‰ ì‹¤íŒ¨: {e}")
