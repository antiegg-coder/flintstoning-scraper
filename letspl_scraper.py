import time
import re
import os
import json
import gspread
from datetime import datetime
from oauth2client.service_account import ServiceAccountCredentials

# ì…€ë ˆë‹ˆì›€ ê´€ë ¨
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# 1. ì„¤ì •
SHEET_URL = "https://docs.google.com/spreadsheets/d/1nKPVCZ6zAOfpqCjV6WfjkzCI55FA9r2yvi9XL3iIneo/edit"
TARGET_GID = 1669656972 
SCRAPE_URL = "https://letspl.me/project?location=KR00&type=00&recruitingType=all&jobD=0207&skill=&interest=&keyword="

# [ìˆ˜ì •ë¨] ê°ì§€í•  ì§€ì—­ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ì •ì˜
REGION_KEYWORDS = [
    "ì„œìš¸", "ê²½ê¸°", "ì¸ì²œ", "ëŒ€ì „", "ëŒ€êµ¬", "ë¶€ì‚°", "ê´‘ì£¼", "ìš¸ì‚°", "ì„¸ì¢…", 
    "ê°•ì›", "ì¶©ë¶", "ì¶©ë‚¨", "ì „ë¶", "ì „ë‚¨", "ê²½ë¶", "ê²½ë‚¨", "ì œì£¼", "ì˜¨ë¼ì¸"
]

def get_google_sheet():
    # ... (ê¸°ì¡´ê³¼ ë™ì¼)
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    creds_dict = json.loads(os.environ['GOOGLE_CREDENTIALS'])
    creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
    client = gspread.authorize(creds)
    
    spreadsheet = client.open_by_url(SHEET_URL)
    worksheet = None
    
    for sheet in spreadsheet.worksheets():
        if str(sheet.id) == str(TARGET_GID):
            worksheet = sheet
            break
            
    if worksheet is None:
        raise Exception(f"GIDê°€ {TARGET_GID}ì¸ ì‹œíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    print(f"ğŸ“‚ ì—°ê²°ëœ ì‹œíŠ¸: {worksheet.title}")
    return worksheet

def get_driver():
    # ... (ê¸°ì¡´ê³¼ ë™ì¼)
    chrome_options = Options()
    chrome_options.add_argument("--headless") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920,1080")
    
    user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    chrome_options.add_argument(f"user-agent={user_agent}")
    
    driver = webdriver.Chrome(options=chrome_options)
    return driver

def get_projects():
    driver = get_driver()
    new_data = []
    today = datetime.now().strftime("%Y-%m-%d")

    try:
        print("ğŸŒ Letspl ì ‘ì† ì¤‘...")
        driver.get(SCRAPE_URL)
        
        wait = WebDriverWait(driver, 15)
        # í”„ë¡œì íŠ¸ ì¹´ë“œ ìš”ì†Œë¥¼ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "a[href^='/project/']")))
        
        time.sleep(3) # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        
        # ëª¨ë“  í”„ë¡œì íŠ¸ ì¹´ë“œ ìš”ì†Œ(<a> íƒœê·¸)ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        elements = driver.find_elements(By.CSS_SELECTOR, "a[href^='/project/']")
        print(f"ğŸ” ë°œê²¬ëœ í”„ë¡œì íŠ¸ ë§í¬ ìˆ˜: {len(elements)}ê°œ")

        for elem in elements:
            try:
                full_url = elem.get_attribute("href")
                
                # URL ìœ íš¨ì„± ê²€ì‚¬
                if not re.search(r'/project/\d+', full_url):
                    continue

                # ================= [ìˆ˜ì •ëœ ë¶€ë¶„ ì‹œì‘] =================
                
                # 1. [ì¤‘ìš”] ì‹¤ì œ ì œëª© ìš”ì†Œë§Œ ì½• ì§‘ì–´ì„œ ê°€ì ¸ì˜¤ê¸°
                # <a> íƒœê·¸ ì•ˆì—ì„œ ì œëª© ì—­í• ì„ í•˜ëŠ” íŠ¹ì • í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ìš”ì†Œë¥¼ ì°¾ìŠµë‹ˆë‹¤.
                # ì£¼ì˜: ì‚¬ì´íŠ¸ êµ¬ì¡°ê°€ ë³€ê²½ë˜ë©´ ì´ ë¶€ë¶„ì˜ ì„ íƒì(class ì´ë¦„ ë“±)ê°€ ë°”ë€” ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                try:
                    # ë ›í”Œì˜ ì¼ë°˜ì ì¸ ì œëª© í´ë˜ìŠ¤ íŒ¨í„´ì„ ì¶”ì •í•˜ì—¬ ì°¾ìŠµë‹ˆë‹¤.
                    # (ì—¬ëŸ¬ ê°œì˜ í´ë˜ìŠ¤ ì´ë¦„ ì¤‘ í•˜ë‚˜ê°€ ì œëª©ì¼ ê²ƒìœ¼ë¡œ ì¶”ì •)
                    title_element = elem.find_element(By.CSS_SELECTOR, "div[class*='title'], h3, h4, strong")
                    title = title_element.text.strip()
                except Exception:
                     # ë§Œì•½ ì œëª© ìš”ì†Œë¥¼ ë³„ë„ë¡œ ì°¾ì§€ ëª»í–ˆë‹¤ë©´, ê¸°ì¡´ ë°©ì‹ì²˜ëŸ¼ ì „ì²´ í…ìŠ¤íŠ¸ì˜ ì²« ì¤„ì„ ì‚¬ìš©í•˜ê±°ë‚˜ ê±´ë„ˆëœë‹ˆë‹¤.
                    # print(f"âš ï¸ ì œëª© ìš”ì†Œë¥¼ íŠ¹ì •í•˜ì§€ ëª»í•´ ê±´ë„ˆëœë‹ˆë‹¤: {full_url}")
                    # continue
                    
                    # (ëŒ€ì•ˆ) ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì²« ë²ˆì§¸ ì˜ë¯¸ ìˆëŠ” ì¤„ì„ ì œëª©ìœ¼ë¡œ ì‚¬ìš© (ê¸°ì¡´ ë°©ì‹ë³´ë‹¤ ë‚˜ìŒ)
                    raw_text = elem.text.strip()
                    lines = [line.strip() for line in raw_text.split('\n') if len(line.strip()) > 2 and "ëª¨ì§‘" not in line]
                    title = lines[0] if lines else ""

                if not title or len(title) < 2:
                    continue
                    
                # 2. ì§€ì—­ ì •ë³´ ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ í™œìš©)
                # ì§€ì—­ ì¶”ì¶œì„ ìœ„í•´ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ ê°€ì ¸ì˜µë‹ˆë‹¤.
                raw_text_for_location = elem.text.strip()
                lines_for_location = raw_text_for_location.split('\n')
                
                location = "ë¯¸ì •"
                for line in lines_for_location:
                    for keyword in REGION_KEYWORDS:
                        if keyword in line:
                            location = keyword
                            break
                    if location != "ë¯¸ì •":
                        break
                        
                # ================= [ìˆ˜ì •ëœ ë¶€ë¶„ ë] =================

                # ë°ì´í„° ì €ì¥
                if not any(d['url'] == full_url for d in new_data):
                    new_data.append({
                        'title': title,
                        'url': full_url,
                        'scraped_at': today,
                        'location': location
                    })
            except Exception as e:
                # print(f"âš ï¸ ê°œë³„ í•­ëª© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}") # ë””ë²„ê¹…ìš©
                continue
                
    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
    finally:
        driver.quit()
            
    print(f"ğŸ¯ ì •ì œëœ ê²Œì‹œë¬¼: {len(new_data)}ê°œ")
    return new_data

def update_sheet(worksheet, data):
    all_values = worksheet.get_all_values()
    
    if not all_values:
        # í—¤ë”ê°€ ì•„ì˜ˆ ì—†ëŠ” ê²½ìš° ìƒì„±
        headers = ['title', 'url', 'scraped_at', 'status', 'location']
        worksheet.append_row(headers)
        all_values = [headers]
    
    headers = all_values[0]

    try:
        idx_title = headers.index('title')
        idx_url = headers.index('url')
        idx_scraped_at = headers.index('scraped_at')
        idx_status = headers.index('status')
        # [ìˆ˜ì •ë¨] location ì»¬ëŸ¼ ì¸ë±ìŠ¤ ì°¾ê¸°
        idx_location = headers.index('location') 
    except ValueError as e:
        print(f"â›” í—¤ë” ì˜¤ë¥˜: ì‹œíŠ¸ 1í–‰ì— {e} ì»¬ëŸ¼ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.")
        print("ğŸ’¡ íŒ: êµ¬ê¸€ ì‹œíŠ¸ 1í–‰ì— 'location' ì´ë¼ê³  ì íŒ ì…€ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
        return

    existing_urls = set()
    for row in all_values[1:]:
        if len(row) > idx_url:
            existing_urls.add(row[idx_url])

    rows_to_append = []
    for item in data:
        if item['url'] in existing_urls:
            continue
            
        # ë¹ˆ í–‰ ìƒì„± (í—¤ë” ê¸¸ì´ë§Œí¼)
        new_row = [''] * len(headers)
        
        # ê°’ ë§¤í•‘
        new_row[idx_title] = item['title']
        new_row[idx_url] = item['url']
        new_row[idx_scraped_at] = item['scraped_at']
        new_row[idx_status] = 'archived'
        new_row[idx_location] = item['location'] # [ìˆ˜ì •ë¨] ì§€ì—­ ê°’ ì…ë ¥
        
        rows_to_append.append(new_row)

    if rows_to_append:
        worksheet.append_rows(rows_to_append)
        print(f"ğŸ’¾ {len(rows_to_append)}ê°œ ì €ì¥ ì™„ë£Œ!")
    else:
        print("â„¹ï¸ ìƒˆë¡œìš´ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    try:
        sheet = get_google_sheet()
        projects = get_projects()
        update_sheet(sheet, projects)
    except Exception as e:
        print(f"ğŸš¨ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
