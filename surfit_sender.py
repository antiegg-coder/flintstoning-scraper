import os
import json
import gspread
from oauth2client.service_account import ServiceAccountCredentials
import pandas as pd
import requests
from bs4 import BeautifulSoup
from openai import OpenAI

# =========================================================
# 1. ì„¤ì • ë° ì¸ì¦
# =========================================================
try:
    print("--- [Surfit Sender] ì‹œì‘ ---")
    
    # í™˜ê²½ë³€ìˆ˜ ë¡œë“œ í™•ì¸
    if 'GOOGLE_CREDENTIALS' not in os.environ:
        raise Exception("í™˜ê²½ë³€ìˆ˜ GOOGLE_CREDENTIALSê°€ ì—†ìŠµë‹ˆë‹¤.")

    json_creds = os.environ['GOOGLE_CREDENTIALS']
    creds_dict = json.loads(json_creds)
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
    client = gspread.authorize(creds)

    # ì‹œíŠ¸ ì—´ê¸° (íŒŒì¼ëª…ì€ ê¸°ì¡´ê³¼ ë™ì¼)
    spreadsheet = client.open('í”Œë¦°íŠ¸ìŠ¤í† ë‹ ì†Œì¬ DB')
    
    # [ìˆ˜ì •ë¨] ì¸ë±ìŠ¤ ë²ˆí˜¸ ëŒ€ì‹  'ì„œí•'ì´ë¼ëŠ” íƒ­ ì´ë¦„ì„ ì§ì ‘ ì°¾ìŠµë‹ˆë‹¤.
    try:
        sheet = spreadsheet.worksheet('ì„œí•')
        print(f"ğŸ“‚ ì—°ê²°ëœ ì‹œíŠ¸: {sheet.title}")
    except gspread.exceptions.WorksheetNotFound:
        print("âŒ 'ì„œí•'ì´ë¼ëŠ” ì´ë¦„ì˜ íƒ­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íƒ­ ì´ë¦„ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        exit()
    except Exception as e:
        print(f"âŒ ì‹œíŠ¸ ë¡œë“œ ì¤‘ ì—ëŸ¬: {e}")
        exit()

    # ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    data = sheet.get_all_values()
    if not data:
        print("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        exit()

    headers = data.pop(0)
    df = pd.DataFrame(data, columns=headers)
    
    # í—¤ë” ê³µë°± ì œê±°
    df.columns = df.columns.str.strip()

    # =========================================================
    # 2. í•„í„°ë§
    # =========================================================
    COL_STATUS = 'status'
    COL_PUBLISH = 'publish'
    COL_TITLE = 'title'
    COL_URL = 'url'

    required_cols = [COL_STATUS, COL_PUBLISH, COL_TITLE, COL_URL]
    for col in required_cols:
        if col not in df.columns:
            print(f"âŒ ì˜¤ë¥˜: ì‹œíŠ¸ì— '{col}' í—¤ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
            exit()

    # ì¡°ê±´: statusëŠ” 'archived', publishëŠ” 'TRUE'
    condition = (df[COL_STATUS].str.strip() == 'archived') & (df[COL_PUBLISH].str.strip() == 'TRUE')
    target_rows = df[condition]

    if target_rows.empty:
        print("â„¹ï¸ ë°œì†¡í•  ëŒ€ìƒ(archived & publish=TRUE)ì´ ì—†ìŠµë‹ˆë‹¤.")
        exit()

    # ì²« ë²ˆì§¸ í–‰ ì„ íƒ
    row = target_rows.iloc[0]
    
    # í–‰ ë²ˆí˜¸ ê³„ì‚° (í—¤ë” ì œì™¸í•œ ë°ì´í„° í”„ë ˆì„ ì¸ë±ìŠ¤ + 2)
    update_row_index = row.name + 2
    
    print(f"â–¶ ì„ íƒëœ í–‰ ë²ˆí˜¸: {update_row_index}")

    # =========================================================
    # 3. ë°ì´í„° ì¶”ì¶œ
    # =========================================================
    project_title = row[COL_TITLE]
    target_url = row[COL_URL]
    
    print(f"â–¶ ì œëª©: {project_title}")
    print(f"â–¶ URL: {target_url}")

    # =========================================================
    # 4. ì›¹ ìŠ¤í¬ë˜í•‘
    # =========================================================
    print("--- ìŠ¤í¬ë˜í•‘ ì‹œì‘ ---")
    headers_ua = {'User-Agent': 'Mozilla/5.0'}
    
    try:
        response = requests.get(target_url, headers=headers_ua, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        article = soup.find('article')
        if article:
            paragraphs = article.find_all('p')
        else:
            paragraphs = soup.find_all('p')
        
        # ë¹ˆ ë¬¸ë‹¨ ì œì™¸ ë° ê³µë°± ì œê±° í›„ ì—°ê²°
        text_list = [p.get_text().strip() for p in paragraphs if p.get_text().strip()]
        full_text = " ".join(text_list)
        
        if len(full_text) < 50:
             print("âš ï¸ ë³¸ë¬¸ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. (ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ê°€ëŠ¥ì„±)")
             
        truncated_text = full_text[:3000]
        
    except Exception as e:
        print(f"âŒ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨:
