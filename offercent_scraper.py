import time
import re
import os
import json
import gspread
from datetime import datetime
from oauth2client.service_account import ServiceAccountCredentials

# ì…€ë ˆë‹ˆì›€ ê´€ë ¨
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# 1. ì„¤ì •
SHEET_URL = "https://docs.google.com/spreadsheets/d/1nKPVCZ6zAOfpqCjV6WfjkzCI55FA9r2yvi9XL3iIneo/edit"

# â–¼ ì‹œíŠ¸ GID (í™•ì¸ í•„ìˆ˜)
TARGET_GID = 639559541
# [ë³€ê²½ë¨] ì˜¤í¼ì„¼íŠ¸ URL
SCRAPE_URL = "https://offercent.co.kr/company-list?jobCategories=0040002%2C0170004"

def get_google_sheet():
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    creds_dict = json.loads(os.environ['GOOGLE_CREDENTIALS'])
    creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
    client = gspread.authorize(creds)
    
    spreadsheet = client.open_by_url(SHEET_URL)
    worksheet = None
    
    for sheet in spreadsheet.worksheets():
        if str(sheet.id) == str(TARGET_GID):
            worksheet = sheet
            break
            
    if worksheet is None:
        raise Exception(f"GIDê°€ {TARGET_GID}ì¸ ì‹œíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    print(f"ğŸ“‚ ì—°ê²°ëœ ì‹œíŠ¸: {worksheet.title}")
    return worksheet

def get_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920,1080")
    
    # ë´‡ íƒì§€ ìš°íšŒ
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option("useAutomationExtension", False)
    
    user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    chrome_options.add_argument(f"user-agent={user_agent}")
    
    driver = webdriver.Chrome(options=chrome_options)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    
    return driver

def get_projects():
    driver = get_driver()
    new_data = []
    today = datetime.now().strftime("%Y-%m-%d")

    try:
        print("ğŸŒ ì˜¤í¼ì„¼íŠ¸ ì ‘ì† ì¤‘...")
        driver.get(SCRAPE_URL)
        
        wait = WebDriverWait(driver, 20)
        # í˜ì´ì§€ ë³¸ë¬¸ ë¡œë”© ëŒ€ê¸°
        wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        time.sleep(5) # ë¦¬ìŠ¤íŠ¸ ë Œë”ë§ ëŒ€ê¸°

        # ìŠ¤í¬ë¡¤ ë‹¤ìš´
        for i in range(3):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
        
        # [ìˆ˜ì •] ì˜¤í¼ì„¼íŠ¸ì˜ ì±„ìš© ê³µê³  ì¹´ë“œëŠ” ë³´í†µ a íƒœê·¸ë¡œ ê°ì‹¸ì ¸ ìˆìŠµë‹ˆë‹¤.
        elements = driver.find_elements(By.TAG_NAME, "a")
        print(f"ğŸ” íƒìƒ‰ëœ ë§í¬ ìˆ˜: {len(elements)}ê°œ")

        for elem in elements:
            try:
                full_url = elem.get_attribute("href")
                if not full_url or full_url == SCRAPE_URL: continue
                
                # í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
                raw_text = elem.text.strip()
                if not raw_text: continue

                # ì¤„ë°”ê¿ˆ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë¶„ë¦¬
                lines = raw_text.split('\n')
                cleaned_lines = [line.strip() for line in lines if line.strip()]
                
                # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ìŠ¤í‚µ (ìµœì†Œ íšŒì‚¬ëª…, ì œëª©ì€ ìˆì–´ì•¼ í•¨)
                if len(cleaned_lines) < 2: continue

                # [ì¤‘ìš”] ì˜¤í¼ì„¼íŠ¸ êµ¬ì¡°ì— ë§ì¶˜ íŒŒì‹± ë¡œì§
                # ë³´í†µ ìˆœì„œ: 1.íšŒì‚¬ëª… 2.ì¹´í…Œê³ ë¦¬/ë¹ˆì¹¸ 3.ì œëª© OR 1.íšŒì‚¬ëª… 2.ì œëª©
                # ì˜ˆ: ['íŒŒë§ˆë¦¬ì„œì¹˜', '[ê²½ë ¥] ì´ì»¤ë¨¸ìŠ¤ ì½˜í…ì¸  ë§ˆì¼€íŒ…', 'D-10']
                
                company = cleaned_lines[0] # ì²« ë²ˆì§¸ ì¤„ì„ íšŒì‚¬ëª…ìœ¼ë¡œ ê°€ì •
                title = ""

                # ë‘ ë²ˆì§¸ ì¤„ë¶€í„° ì œëª© ì°¾ê¸° (ë³´í†µ ë‘ ë²ˆì§¸ ì¤„ì´ ì œëª©)
                if len(cleaned_lines) >= 2:
                    title = cleaned_lines[1]
                
                # ì œëª© ê²€ì¦: ë§Œì•½ 2ë²ˆì§¸ ì¤„ì´ ì¹´í…Œê³ ë¦¬(ì˜ˆ: 'ë§ˆì¼€íŒ…')ê³  3ë²ˆì§¸ ì¤„ì´ ì§„ì§œ ì œëª©ì¼ ê²½ìš° ëŒ€ë¹„
                # ì œëª©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜(4ê¸€ì ì´í•˜) íŠ¹ì • ë‹¨ì–´ë©´ ë‹¤ìŒ ì¤„ì„ ì œëª©ìœ¼ë¡œ ë´…ë‹ˆë‹¤.
                if len(title) < 4 and len(cleaned_lines) >= 3:
                     title = cleaned_lines[2]

                # ì œëª©ì— ëŒ€ê´„í˜¸ [ ] ê°€ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ ì œëª©ì¼ í™•ë¥ ì´ ë†’ìŒ (ì˜ˆ: [ê²½ë ¥])
                # í˜¹ì€ íšŒì‚¬ëª…ì´ ë„ˆë¬´ ê¸¸ë©´(ê³µê³  ì œëª©ì´ ì²« ì¤„ì— ì™”ì„ ê°€ëŠ¥ì„±) ìŠ¤ì™‘ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥í•˜ë‚˜,
                # í˜„ì¬ëŠ” "íŒŒë§ˆë¦¬ì„œì¹˜"ê°€ ë¨¼ì € ë‚˜ì˜¤ëŠ” íŒ¨í„´ì„ ìš°ì„ í•©ë‹ˆë‹¤.

                # í•„í„°ë§: ë§ˆê°ì¼, D-Day, ì§€ì—­ëª… ë“±ì´ ì œëª©ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” ê²ƒì„ ë°©ì§€
                if title.startswith("D-") or "ë§ˆê°" in title or title.endswith("êµ¬"):
                     if len(cleaned_lines) >= 3:
                         title = cleaned_lines[2]

                # ê²°ê³¼ê°€ ìœ íš¨í•œì§€ í™•ì¸ í›„ ì¶”ê°€
                if len(title) > 2 and len(company) > 1:
                    # ì¤‘ë³µ URL ì²´í¬
                    if not any(d['url'] == full_url for d in new_data):
                        # ë””ë²„ê¹…ìš© ì¶œë ¥ (ë¡œê·¸ì—ì„œ í™•ì¸ ê°€ëŠ¥)
                        # print(f"  -> ì¶”ì¶œ: {company} / {title}")
                        
                        new_data.append({
                            'title': title,
                            'company': company,
                            'url': full_url,
                            'scraped_at': today
                        })
            except Exception:
                continue
                
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì—ëŸ¬: {e}")
    finally:
        driver.quit()
            
    print(f"ğŸ¯ ìˆ˜ì§‘ëœ ê³µê³ : {len(new_data)}ê°œ")
    # ìƒ˜í”Œ ë°ì´í„° 3ê°œë§Œ ì¶œë ¥í•´ì„œ í™•ì¸
    if len(new_data) > 0:
        print("ğŸ“Š [ìƒ˜í”Œ ë°ì´í„° í™•ì¸]")
        for i in range(min(3, len(new_data))):
            print(f"   ì œëª©: {new_data[i]['title']} | íšŒì‚¬: {new_data[i]['company']}")

    return new_data

def update_sheet(worksheet, data):
    all_values = worksheet.get_all_values()
    
    if not all_values:
        headers = ['title', 'company', 'url', 'scraped_at', 'status']
        worksheet.append_row(headers)
        all_values = [headers]
    
    headers = all_values[0]
    try:
        idx_title = headers.index('title')
        idx_company = headers.index('company')
        idx_url = headers.index('url')
        idx_scraped_at = headers.index('scraped_at')
        idx_status = headers.index('status')
    except:
        print("â›” í—¤ë” ì˜¤ë¥˜")
        return

    existing_urls = set()
    if len(all_values) > 1:
        for row in all_values[1:]:
            if len(row) > idx_url:
                existing_urls.add(row[idx_url])

    rows_to_append = []
    empty_row = [''] * len(headers)

    for item in data:
        if item['url'] in existing_urls:
            continue
        new_row = empty_row.copy()
        new_row[idx_title] = item['title']
        new_row[idx_company] = item['company']
        new_row[idx_url] = item['url']
        new_row[idx_scraped_at] = item['scraped_at']
        new_row[idx_status] = 'archived'
        rows_to_append.append(new_row)

    if rows_to_append:
        worksheet.append_rows(rows_to_append)
        print(f"ğŸ’¾ {len(rows_to_append)}ê°œ ì €ì¥ ì™„ë£Œ")
    else:
        print("â„¹ï¸ ì‹ ê·œ ê³µê³  ì—†ìŒ")

if __name__ == "__main__":
    try:
        sheet = get_google_sheet()
        projects = get_projects()
        update_sheet(sheet, projects)
    except Exception as e:
        print(f"ğŸš¨ ì‹¤íŒ¨: {e}")
