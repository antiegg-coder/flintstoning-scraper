import time
import re
import os
import json
import gspread
from datetime import datetime
from oauth2client.service_account import ServiceAccountCredentials

# ì…€ë ˆë‹ˆì›€
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# 1. ì„¤ì •
SHEET_URL = "https://docs.google.com/spreadsheets/d/1nKPVCZ6zAOfpqCjV6WfjkzCI55FA9r2yvi9XL3iIneo/edit"
TARGET_GID = 639559541
SCRAPE_URL = "https://offercent.co.kr/company-list?jobCategories=0040002%2C0170004"

def get_google_sheet():
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    creds_dict = json.loads(os.environ['GOOGLE_CREDENTIALS'])
    creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
    client = gspread.authorize(creds)
    
    spreadsheet = client.open_by_url(SHEET_URL)
    worksheet = None
    for sheet in spreadsheet.worksheets():
        if str(sheet.id) == str(TARGET_GID):
            worksheet = sheet
            break
    if worksheet is None:
        raise Exception(f"GIDê°€ {TARGET_GID}ì¸ ì‹œíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return worksheet

def get_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920,1080")
    
    # ë´‡ íƒì§€ ìš°íšŒ ì„¤ì •
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option("useAutomationExtension", False)
    
    user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    chrome_options.add_argument(f"user-agent={user_agent}")
    
    driver = webdriver.Chrome(options=chrome_options)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    
    return driver

def get_projects():
    driver = get_driver()
    new_data = []
    today = datetime.now().strftime("%Y-%m-%d")
    collected_urls = set()

    try:
        print("ğŸŒ ì˜¤í¼ì„¼íŠ¸ ì ‘ì† ì¤‘...")
        driver.get(SCRAPE_URL)
        
        # [ìˆ˜ì •] ë‹¨ìˆœ body ëŒ€ê¸°ê°€ ì•„ë‹ˆë¼, ì‹¤ì œ 'a' íƒœê·¸ê°€ ëœ° ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼ (ìµœëŒ€ 30ì´ˆ)
        wait = WebDriverWait(driver, 30)
        try:
            wait.until(EC.presence_of_element_located((By.TAG_NAME, "a")))
            print("âœ… í˜ì´ì§€ ë¡œë”© ê°ì§€ë¨ (ë§í¬ ìš”ì†Œ í™•ì¸)")
        except:
            print("âš ï¸ ê²½ê³ : 30ì´ˆ ë™ì•ˆ ë§í¬(a íƒœê·¸)ê°€ í•˜ë‚˜ë„ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   -> í˜ì´ì§€ê°€ ì°¨ë‹¨ë˜ì—ˆê±°ë‚˜, ë¡œë”©ì´ ë§¤ìš° ëŠë¦½ë‹ˆë‹¤.")
            print(f"   -> í˜„ì¬ URL: {driver.current_url}")
            print(f"   -> í˜ì´ì§€ ì†ŒìŠ¤ ì¼ë¶€: {driver.page_source[:500]}") # ì°¨ë‹¨ ë©”ì‹œì§€ í™•ì¸ìš©

        # ë¡œë”© í›„ ì•ˆì „í•˜ê²Œ ì¡°ê¸ˆ ë” ëŒ€ê¸°
        time.sleep(5) 

        # ---------------------------------------------------------
        # ìˆ˜ì§‘ í•¨ìˆ˜
        # ---------------------------------------------------------
        def scrape_current_view(debug_mode=False):
            elements = driver.find_elements(By.TAG_NAME, "a")
            count = 0
            
            # ë””ë²„ê¹…: ì°¾ì€ ìš”ì†Œê°€ 0ê°œë©´ ë¡œê·¸ ì¶œë ¥
            if len(elements) == 0 and debug_mode:
                print("   âš ï¸ í˜„ì¬ í™”ë©´ì—ì„œ 'a' íƒœê·¸ë¥¼ í•˜ë‚˜ë„ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤.")

            BAD_KEYWORDS = ["ì±„ìš© ì¤‘ì¸ ê³µê³ ", "ì±„ìš©ë§ˆê°", "ë§ˆê°ì„ë°•", "ìƒì‹œì±„ìš©", "NEW", "D-"]

            for elem in elements:
                try:
                    full_url = elem.get_attribute("href")
                    if not full_url or full_url == SCRAPE_URL or full_url in collected_urls: 
                        continue
                    
                    raw_text = elem.text.strip()
                    if not raw_text: continue

                    lines = raw_text.split('\n')
                    cleaned_lines = []
                    
                    for line in lines:
                        text = line.strip()
                        if not text: continue
                        
                        is_bad = False
                        for bad in BAD_KEYWORDS:
                            if bad in text:
                                is_bad = True
                                break
                        if not is_bad:
                            cleaned_lines.append(text)

                    if len(cleaned_lines) < 2: continue

                    company = cleaned_lines[0]
                    title = cleaned_lines[1]

                    # ì œëª© ë³´ì • ë¡œì§
                    if len(title) <= 3 and len(cleaned_lines) > 2:
                        title = cleaned_lines[2]

                    if len(title) > 1 and len(company) > 1:
                        new_data.append({
                            'title': title,
                            'company': company,
                            'url': full_url,
                            'scraped_at': today
                        })
                        collected_urls.add(full_url)
                        count += 1
                except:
                    continue
            return count
        # ---------------------------------------------------------

        print("â¬‡ï¸ ìŠ¤í¬ë¡¤ê³¼ ë™ì‹œì— ìˆ˜ì§‘ ì‹œì‘...")
        
        # [1] ì²« í™”ë©´ ìˆ˜ì§‘ (ë””ë²„ê·¸ ëª¨ë“œ ì¼œê¸°)
        first_count = scrape_current_view(debug_mode=True)
        print(f"   ğŸš€ ì²« í™”ë©´ ìˆ˜ì§‘ ê²°ê³¼: {first_count}ê°œ")

        last_height = driver.execute_script("return document.body.scrollHeight")
        scroll_count = 0
        
        while True:
            # ìŠ¤í¬ë¡¤ ë‹¤ìš´
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(3) # ë¡œë”© ëŒ€ê¸°
            
            # ìŠ¤í¬ë¡¤ í›„ ìˆ˜ì§‘
            found = scrape_current_view()
            
            new_height = driver.execute_script("return document.body.scrollHeight")
            scroll_count += 1
            
            # ë¡œê·¸ë¥¼ ë„ˆë¬´ ë§ì´ ì¶œë ¥í•˜ì§€ ì•Šë„ë¡ 3ë²ˆë§ˆë‹¤, í˜¹ì€ ìˆ˜ì§‘ë˜ì—ˆì„ ë•Œë§Œ ì¶œë ¥
            if found > 0 or scroll_count % 3 == 0:
                print(f"   ...ìŠ¤í¬ë¡¤ {scroll_count}íšŒ (ì´ë²ˆ í„´ {found}ê°œ ì¶”ê°€ / ëˆ„ì  {len(new_data)}ê°œ)")

            if new_height == last_height:
                # ë§ˆì§€ë§‰ í™•ì¸ ì‚¬ì‚´
                scrape_current_view()
                print("ğŸ í˜ì´ì§€ ë ë„ë‹¬")
                break
                
            last_height = new_height
            
            # [ì•ˆì „ì¥ì¹˜] ë¬´í•œë£¨í”„ ë°©ì§€ (ìµœëŒ€ 50ë²ˆ ìŠ¤í¬ë¡¤)
            if scroll_count > 50:
                print("âš ï¸ ë„ˆë¬´ ë§ì´ ìŠ¤í¬ë¡¤ë˜ì–´ ê°•ì œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
                
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì—ëŸ¬: {e}")
    finally:
        driver.quit()
            
    print(f"ğŸ¯ ìµœì¢… ìˆ˜ì§‘ëœ ê³µê³ : {len(new_data)}ê°œ")
    
    if len(new_data) > 0:
        print("ğŸ“Š [ìƒ˜í”Œ ë°ì´í„°]")
        for i in range(min(3, len(new_data))):
             print(f"   ì œëª©: {new_data[i]['title']} / íšŒì‚¬: {new_data[i]['company']}")
    else:
        print("â›” ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ 0ê°œì…ë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    return new_data

def update_sheet(worksheet, data):
    all_values = worksheet.get_all_values()
    
    if not all_values:
        headers = ['title', 'company', 'url', 'scraped_at', 'status']
        worksheet.append_row(headers)
        all_values = [headers]
    
    headers = all_values[0]
    try:
        idx_title = headers.index('title')
        idx_company = headers.index('company')
        idx_url = headers.index('url')
        idx_scraped_at = headers.index('scraped_at')
        idx_status = headers.index('status')
    except:
        print("â›” í—¤ë” ì˜¤ë¥˜")
        return

    existing_urls = set()
    if len(all_values) > 1:
        for row in all_values[1:]:
            if len(row) > idx_url:
                existing_urls.add(row[idx_url])

    rows_to_append = []
    empty_row = [''] * len(headers)

    for item in data:
        if item['url'] in existing_urls:
            continue
        new_row = empty_row.copy()
        new_row[idx_title] = item['title']
        new_row[idx_company] = item['company']
        new_row[idx_url] = item['url']
        new_row[idx_scraped_at] = item['scraped_at']
        new_row[idx_status] = 'archived'
        rows_to_append.append(new_row)

    if rows_to_append:
        worksheet.append_rows(rows_to_append)
        print(f"ğŸ’¾ {len(rows_to_append)}ê°œ ì €ì¥ ì™„ë£Œ!")
    else:
        print("â„¹ï¸ ì €ì¥í•  ìƒˆë¡œìš´ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    try:
        sheet = get_google_sheet()
        projects = get_projects()
        update_sheet(sheet, projects)
    except Exception as e:
        print(f"ğŸš¨ ì‹¤íŒ¨: {e}")
