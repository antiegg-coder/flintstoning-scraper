def get_projects():
    driver = get_driver()
    new_data = []
    today = datetime.now().strftime("%Y-%m-%d")

    try:
        print("ğŸŒ ì˜¤í¼ì„¼íŠ¸ ì ‘ì† ì¤‘...")
        driver.get(SCRAPE_URL)
        
        wait = WebDriverWait(driver, 20)
        wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        time.sleep(5) 

        # ìŠ¤í¬ë¡¤ ë‹¤ìš´
        for i in range(3):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
        
        elements = driver.find_elements(By.TAG_NAME, "a")
        print(f"ğŸ” ë°œê²¬ëœ ì „ì²´ ë§í¬ ìˆ˜: {len(elements)}ê°œ")

        # ë¬´ì‹œí•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        SKIP_KEYWORDS = ["ì±„ìš© ì¤‘ì¸ ê³µê³ ", "ì±„ìš©ë§ˆê°", "ë§ˆê°ì„ë°•", "ìƒì‹œì±„ìš©", "NEW", "D-"]

        for idx, elem in enumerate(elements):
            try:
                full_url = elem.get_attribute("href")
                if not full_url or full_url == SCRAPE_URL: continue
                
                raw_text = elem.text.strip()
                if not raw_text: continue

                lines = raw_text.split('\n')
                
                # [ë””ë²„ê¹…] ì²˜ìŒ 5ê°œ ë§í¬ëŠ” ë¬´ì¡°ê±´ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ì¶œë ¥í•´ì„œ í™•ì¸
                if idx < 5:
                    print(f"----- [ë§í¬ {idx}] ì›ë³¸ í…ìŠ¤íŠ¸ ë¶„ì„ -----")
                    print(f"URL: {full_url}")
                    print(f"ì¤„ë°”ê¿ˆ í¬í•¨ ë‚´ìš©: {lines}")

                cleaned_lines = []
                for line in lines:
                    text = line.strip()
                    if not text: continue
                    
                    # í‚¤ì›Œë“œê°€ 'í¬í•¨'ë§Œ ë˜ì–´ë„ ê³¼ê°íˆ ì‚­ì œ
                    is_bad = False
                    for kw in SKIP_KEYWORDS:
                        if kw in text:
                            is_bad = True
                            break
                    
                    if not is_bad:
                        cleaned_lines.append(text)
                
                if idx < 5:
                    print(f"í•„í„°ë§ í›„ ë‚´ìš©: {cleaned_lines}")

                # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ìŠ¤í‚µ
                if len(cleaned_lines) < 2:
                    continue

                # ìˆœì„œ ê²°ì • ë¡œì§ (íšŒì‚¬ëª… vs ì œëª©)
                # ì˜¤í¼ì„¼íŠ¸ëŠ” ë³´í†µ [íšŒì‚¬ëª…, ì œëª©] ìˆœì„œì„
                company = cleaned_lines[0]
                title = cleaned_lines[1]
                
                # ë§Œì•½ ì œëª©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´(3ê¸€ì ì´í•˜) ê·¸ ë‹¤ìŒ ì¤„ì´ ì œëª©ì¼ ìˆ˜ ìˆìŒ
                if len(title) <= 3 and len(cleaned_lines) > 2:
                    title = cleaned_lines[2]

                # ìµœì¢… ì €ì¥
                if len(title) > 1 and len(company) > 1:
                    # ì¤‘ë³µ ì²´í¬
                    if not any(d['url'] == full_url for d in new_data):
                        new_data.append({
                            'title': title,
                            'company': company,
                            'url': full_url,
                            'scraped_at': today
                        })
                        # ìˆ˜ì§‘ ì„±ê³µ ì‹œ ë¡œê·¸ ì¶œë ¥
                        # print(f"  âœ… ìˆ˜ì§‘ ì„±ê³µ: {title} / {company}")

            except Exception as e:
                print(f"âš ï¸ íŒŒì‹± ì—ëŸ¬: {e}")
                continue
                
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì—ëŸ¬: {e}")
    finally:
        driver.quit()
            
    print(f"ğŸ¯ ìµœì¢… ìˆ˜ì§‘ëœ ê³µê³ : {len(new_data)}ê°œ")
    return new_data
